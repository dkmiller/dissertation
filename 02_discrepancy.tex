% !TEX root = main.tex

\chapter{Discrepancy}





\section{Definitions and first results}

The discrepancy (also known as the Kolmogorov--Smirnov statistic) is a way of 
measuring how closely sample data fits a predicted distribution. It has many 
applications in computer science and statistics, but here we will focus on only 
the basic known properties, as well as how discrepancy changes when sequences 
are tweaked and/or combined. 

Discrepancy will be defined for measures on the $d$-dimensional half-open box 
$[0,\infty)^d$. For vectors $x,y\in [0,\infty)^d$, we say $x<y$ if 
$x_1<y_1$,\dots,$x_d<y_d$, and in that case write $[x,y)$ for the half-open 
box $[x_1,y_1)\times \cdots \times [x_d,y_d)$. 

\begin{definition}
Let $\mu, \nu$ be probability measures on $[0,\infty)^d$. The 
\emph{discrepancy} of $\mu$ with respect to $\nu$ is 
\[
	\disc(\mu,\nu) = \sup_{x < y} \left| \mu[x,y) - \nu[x,y)\right| ,
\]
where $x<y$ range over $[0,\infty)^d$.

The \emph{star discrepancy} of $\mu$ with respect to $\nu$ is 
\[
	\disc^\star(\mu,\nu) = \sup_{0<y} \left| \mu[0,y) - \nu[0,y)\right| ,
\]
where $y$ ranges over $[0,\infty)^d$. 
\end{definition}

\begin{lemma}
Let $\mu,\nu$ be Borel measures on $\bR^d$. Then 
\[
	\disc^\star(\mu,\nu) \leqslant \disc(\mu,\nu) \leqslant 2^d \disc^\star(\mu,\nu) .
\]
\end{lemma}
\begin{proof}
The first inequality holds because the supremum defining the discrepancy is 
taken over a larger set than that defining star discrepancy. To prove the 
second inequality, let $x<y$ be in $[0,\infty)^d$. For 
$S\subset \{1,\dots,d\}$, let 
\[
	I_S = \{ t \in [0,y) : t_i < x_i \text{ for all }i\in S\} .
\]
The inclusion-exclusion principle for measures tells us that: 
\[
	\mu[x,y) = \sum_{S\subset \{1,\dots,d\}} (-1)^{\# S} \mu(I_S) ,
\]
and similarly for $\nu$. Since each of the $I_S$ are ``half-open boxes'' 
we know that $|\mu(I_S) - \nu(I_S)| \leqslant \disc^\star(\mu,\nu)$. It 
follows that 
\[
	|\mu[x,y) - \nu[x,y)| \leqslant \sum_{S\subset \{1,\dots,d\}} |\mu(I_S) - \nu(I_S)| \leqslant 2^d \disc^\star(\mu,\nu) .
\]
\end{proof}

We are usually interested in comparing empirical measures and their conjectured 
distribution. Namely, let $\bx = \{x_p\}$ be a sequence in $[0,\infty)^d$ 
indexed by the prime numbers, and $\mu$ a Borel measure on $[0\infty)^d$. For 
any real number $N\geqslant 2$, we write $\bx^N$ for the empirical measure 
given by 
\[
	\bx^N(S) = \frac{1}{\pi(N)} \sum_{p\leqslant N} \delta_{x_p}(S) = \frac{\# \{p\leqslant N : x_p\in S\}}{\pi(N)} .
\]
Also, we write $\bx_{\geqslant N}$ for the truncated sequence 
$(x_p)_{p\geqslant N}$, and similarly for $\bx_{\leqslant N}$, etc. In this 
context, 
\[
	\disc^\star(\bx^N,\nu) = \sup_{y\in [0,\infty)^d} \left| \frac{\# \{p\leqslant N : x_p \in [0,y)\}}{\pi(N)} - \int_{[0,y)} \, \dd\nu\right| .
\]

If the measure $\nu$ is only defined on a subset of $[0,\infty)^d$, we will 
tacitly extend it by zero. Moreover, if the sequence $\bx$ actually lies in a 
torus $(\bR/a \bZ)^d$, we identify that torus with the 
$[0,a)^d\subset [0,\infty)^d$. If $\nu$ is the Lebesgue measure (on 
$[0,\infty)^d$) or the normalized Haar measure on the torus, we write 
$\disc^\star(\bx^N)$ in place of $\disc^\star(\bx^N, \nu)$. 

Sometimes the sequence $\bx$ will not be indexed by the prime numbers, but 
rather by some other discrete subset of $\bR^+$. In that case we will still 
use the notations $\bx^N$, $\bx_{\geqslant N}$, etc., keeping in mind that 
$\pi(N)$ is replaced by $\#\{\textnormal{indices }\leqslant N\}$. 





\section{The Koksma--Hlawka inequality}

% TODO
Basically just summarize the paper \cite{okten-1999}. 


\begin{theorem}[Koksma--Hlawka]
Todo.
\end{theorem}





\section{Comparing sequences}

\begin{lemma}
Let $\bx$ and $\by$ be sequences in $[0,\infty)$. Suppose 
$\nu = f\cdot \lambda$ for $f$ a bounded continuous function and $\lambda$ the 
Lebesgue measure. Then 
\[
	\left|\disc^\star(\bx^N, \nu) - \disc^\star(\by^N,\nu)\right| \leqslant \|f\|_\infty \epsilon + \disc^\star(\bx^N,\nu) + \frac{\#\{ p\leqslant N : \|x_p - y_p\|_\infty \geqslant \epsilon\}}{\pi(N)} .
\]
\end{lemma}
\begin{proof}
Let $\epsilon>0$ and $t\in [0,\infty)$ be arbitrary. For all $p\leqslant N$ 
such that $y_p<t$, either $x_p < t+\epsilon$ or 
$\|x_p - y_p\|_\infty \geqslant \epsilon$. It follows that 
\[
	\by^N[0,t) \leqslant \bx^N[0,t+\epsilon) + \frac{\#\{ p\leqslant N : \|x_p - y_p\|_\infty \geqslant \epsilon\}}{\pi(N)} .
\]
Moreover, we trivially have 
\[
	\left| \bx^N[0,t+\epsilon) - \nu[0,t+\epsilon)\right| \leqslant \disc^\star(\bx^N,\nu) .
\]
Putting these together, we get: 
\begin{align*}
	\by^N[0,t) - \nu[0,t) 
		&\leqslant \bx^N[0,t+\epsilon) - \nu[0,t) + \frac{\#\{ p\leqslant N : \|x_p - y_p\|_\infty \geqslant \epsilon\}}{\pi(N)} \\
		&\leqslant \nu[t,t+\epsilon) + \disc^\star(\bx^N,\nu) + \frac{\#\{ p\leqslant N : \|x_p - y_p\|_\infty \geqslant \epsilon\}}{\pi(N)} \\
		&\leqslant \|f\|_\infty \epsilon + \disc^\star(\bx^N,\nu) + \frac{\#\{ p\leqslant N : \|x_p - y_p\|_\infty \geqslant \epsilon\}}{\pi(N)} 
\end{align*}
as desired. 
\end{proof}

\begin{lemma}
Let $\sigma$ be an isometry of $\bR$, and $\bx$ a sequence in $[0,\infty)$ 
such that $\sigma(\bx)$ is also in $[0,\infty)$. Let $\nu$ be an absolutely 
continuous measure on $[0,\infty)$ such that $\sigma_\ast \nu$ is also 
supported on $[0,\infty)$. Then 
\[
	\left|\disc(\bx^N, \nu) - \disc(\sigma_\ast \bx^N, \sigma_\ast \nu)\right| \leqslant \frac{2}{\pi(N)} .
\]
\end{lemma}
\begin{proof}
Every isometry of $\bR$ is a combination of translations and reflections. 
The statement is clear with translations (the two discrepancies are equal). So, 
suppose $\sigma(t) = a - t$ for some $a>0$. Since $\nu$ is absolutely 
continuous, $\nu\{t\}=0$ for all $t\geqslant 0$. In particular, 
$\nu[s,t) = \nu(s,t]$. In contrast, $\bx^N\{t\}\leqslant \pi(N)^{-1}$. For any 
interval $[s,t)$ in $[0,\infty)$, we know that 
\[
	\left| \bx^N[s,t) - \bx^N(s,t]\right| \leqslant \frac{2}{\pi(N)}  ,
\]
hence 
\[
	\left| \bx^N[s,t) - \nu[s,t) - (\sigma_\ast \bx^N)[a-t,a-s) - (\sigma_\ast \nu)[a-t,a-s)\right| \leqslant \frac{2}{\pi(N)} .
\]
This proves the result. 
\end{proof}





\section{Combining sequences}

\begin{definition}
Let $\bx$ and $\by$ be sequences in $[0,\infty)^d$. We write $\bx\wr\by$ for 
the interleaved sequence 
\[
	(x_2,y_2,x_3,y_3,x_5,y_5,\dots,x_p,y_p,\dots) .
\]
\end{definition}

For the interleaved sequence $\bx\wr\by$, we write $(\bx\wr\by)^N$ for the 
empirical measure 
\[
	(\bx\wr\by)^N = \frac{1}{2\pi(N)} \sum_{p\leqslant N} \delta_{x_p} + \delta_{y_p} .
\]

\begin{theorem}
Let $I$ and $J$ be disjoint open boxes in $[0,\infty)^d$, and let $\mu$, 
$\nu$ be absolutely continuous probability measures on $I$ and $J$, 
respectively. Let $\bx$ be a sequence in $I$ and $\by$ be a sequence in $J$. 
Then 
\[
	\max\{\disc(\bx^N,\mu),\disc(\by^N,\nu)\} \leqslant \disc((\bx\wr\by)^N, \mu+\nu) \leqslant \disc(\bx^N,\mu) + \disc(\by^N,\nu)
\]
\end{theorem}
\begin{proof}
Any half-open box in $[0,\infty)^d$ can be split by a coordinate 
hyperplane into two disjoint half-open boxes $[a,b)\sqcup [s,t)$, each of which 
intersects at most one of $I$ and $J$. We may assume that 
$[a,b)\cap J=\varnothing$ and $[s,t)\cap I = \varnothing$. Then 
\begin{align*}
	\left| (\bx\wr\by)^N([a,b)\sqcup [s,t)) - (\mu+\nu)([a,b)\sqcup[s,t))\right| 
		&\leqslant |\bx^N[a,b) - \mu[a,b)| + |\by^N[s,t) - \nu[s,t)| \\
		&\leqslant \disc(\bx^N,\mu) + \disc(\by^N,\nu) .
\end{align*}
This yields the second inequality in the statement of the theorem. To see the 
first, assume that the maximum discrepancy is $\disc(\bx^N,\mu)$, and let 
$[s,t)$ be a half-open box such that $|\bx^N[s,t) - \mu[s,t)|$ is within an 
arbitrary $\epsilon$ of $\disc(\bx^N,\mu)$. We can assume that $[s,t)$ does not 
intersect $J$, and thus 
\[
	\left|(\bx\wr\by)^N[s,t) - (\mu+\nu)[s,t)\right| = |\bx^N[s,t) - \mu[s,t)| ,
\]
which yields the result. 
\end{proof}
