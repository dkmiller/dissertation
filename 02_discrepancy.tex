% !TEX root = main.tex

\chapter{Discrepancy}





\section{Equidistribution}

The discrepancy (also known as the Kolmogorov--Smirnov statistic) is a way of 
measuring how closely sample data fits a predicted distribution. It has many 
applications in computer science and statistics, but here we will focus on only 
the basic known properties, as well as how discrepancy changes when sequences 
are tweaked and/or combined. 

First, recall that the discrepancy is a way of sharpening the ``soft'' 
convergence results of, say \cite[A.1]{serre-1989}. Let $X$ be a compact 
topological space, $\{x_p\}$ a sequence of points in $X$ indexed by the prime 
numbers. 

\begin{definition}
Let $\mu$ be a continuous probability measure on $X$. The sequence $\{x_p\}$ is 
\emph{equidistributed} with respect to $\mu$ if for all $f\in C(X)$, we have 
\[
	\lim_{x\to \infty} \frac{1}{\pi(x)} \sum_{p\leqslant x} f(x_p) \to \int f\, \dd \mu .
\]
\end{definition}

In other words, $\{x_p\}$ is $\mu$-equidistributed if the empirical measures 
$P_x = \frac{1}{\pi(x)} \sum_{p\leqslant x} \delta_{x_p}$ converge to $\mu$ in 
the weak topology. It is easy to see that $\{x_p\}$ is $\mu$-equidistributed if 
and only if $\left| \sum_{p\leqslant x} f(x_p)\right| = o(x)$ for all 
continuous $f$ having $\int f\, \dd\mu = 0$. In fact, one can restrict to a 
set of $f$ which generate a dense subpace of $C(X)^{\mu =0}$. 

In the discussion in \cite[A.1]{serre-1989}, $X$ is the space of conjugacy 
classes in a compact Lie group, and $f$ is allowed to range over the characters 
of irreducible, nontrivial representations of the group. In this section, we 
will show that the entire discussion can be generalized to a much broader class 
of \emph{strange Dirichlet series}, which are of the form 
\[
	L_f(\{x_p\},s) = \prod_p \frac{1}{1-f(x_p)p^{-s}} .
\]
A useful, but not too well known, result, is that we in fact can consider 
functions $f$ which are only continuous almost everywhere. 

\begin{theorem}
Let $X$ be a compact separable metric space with no isolated points. Let $\mu$ 
be a Borel measure on $X$ and let $f\colon X\to \bC$ be bounded and measurable. 
Then $f$ is continuous almost everywhere if and only if 
\[
	\lim_{x\to \infty} \frac{1}{\pi(x)} \sum_{p\leqslant x} f(x_p) = \int f\, \dd\mu
\]
for all $\mu$-equidistributed sequences $\{x_p\}$. 
\end{theorem}
\begin{proof}
This follows immediately from the proof of \cite[Th.~1]{mazzone-1995}
\end{proof}





\section{Definitions and first results}

We will define discrepancy for measures on the $d$-dimensional half-open box 
$[0,\infty)^d$. For vectors $x,y\in [0,\infty)^d$, we say $x<y$ if 
$x_1<y_1$,\dots,$x_d<y_d$, and in that case write $[x,y)$ for the half-open 
box $[x_1,y_1)\times \cdots \times [x_d,y_d)$. 

\begin{definition}
Let $\mu, \nu$ be probability measures on $[0,\infty)^d$. The 
\emph{discrepancy} of $\mu$ with respect to $\nu$ is 
\[
	\disc(\mu,\nu) = \sup_{x < y} \left| \mu[x,y) - \nu[x,y)\right| ,
\]
where $x<y$ range over $[0,\infty)^d$.

The \emph{star discrepancy} of $\mu$ with respect to $\nu$ is 
\[
	\disc^\star(\mu,\nu) = \sup_{0<y} \left| \mu[0,y) - \nu[0,y)\right| ,
\]
where $y$ ranges over $[0,\infty)^d$. 
\end{definition}

\begin{lemma}
Let $\mu,\nu$ be Borel measures on $\bR^d$. Then 
\[
	\disc^\star(\mu,\nu) \leqslant \disc(\mu,\nu) \leqslant 2^d \disc^\star(\mu,\nu) .
\]
\end{lemma}
\begin{proof}
The first inequality holds because the supremum defining the discrepancy is 
taken over a larger set than that defining star discrepancy. To prove the 
second inequality, let $x<y$ be in $[0,\infty)^d$. For 
$S\subset \{1,\dots,d\}$, let 
\[
	I_S = \{ t \in [0,y) : t_i < x_i \text{ for all }i\in S\} .
\]
The inclusion-exclusion principle for measures tells us that: 
\[
	\mu[x,y) = \sum_{S\subset \{1,\dots,d\}} (-1)^{\# S} \mu(I_S) ,
\]
and similarly for $\nu$. Since each of the $I_S$ are ``half-open boxes'' 
we know that $|\mu(I_S) - \nu(I_S)| \leqslant \disc^\star(\mu,\nu)$. It 
follows that 
\[
	|\mu[x,y) - \nu[x,y)| \leqslant \sum_{S\subset \{1,\dots,d\}} |\mu(I_S) - \nu(I_S)| \leqslant 2^d \disc^\star(\mu,\nu) .
\]
For a discussion and related context, see 
\cite[Ch.~2 Ex.~1.2]{kuipers-niederreiter-1974}. 
\end{proof}

We are usually interested in comparing empirical measures and their conjectured 
distribution. Namely, let $\bx = \{x_p\}$ be a sequence in $[0,\infty)^d$ 
indexed by the prime numbers, and $\mu$ a Borel measure on $[0\infty)^d$. For 
any real number $N\geqslant 2$, we write $\bx^N$ for the empirical measure 
given by 
\[
	\bx^N(S) = \frac{1}{\pi(N)} \sum_{p\leqslant N} \delta_{x_p}(S) = \frac{\# \{p\leqslant N : x_p\in S\}}{\pi(N)} .
\]
Also, we write $\bx_{\geqslant N}$ for the truncated sequence 
$(x_p)_{p\geqslant N}$, and similarly for $\bx_{\leqslant N}$, etc. In this 
context, 
\[
	\disc^\star(\bx^N,\nu) = \sup_{y\in [0,\infty)^d} \left| \frac{\# \{p\leqslant N : x_p \in [0,y)\}}{\pi(N)} - \int_{[0,y)} \, \dd\nu\right| .
\]

If the measure $\nu$ is only defined on a subset of $[0,\infty)^d$, we will 
tacitly extend it by zero. Moreover, if the sequence $\bx$ actually lies in a 
torus $(\bR/a \bZ)^d$, we identify that torus with the 
$[0,a)^d\subset [0,\infty)^d$. If $\nu$ is the Lebesgue measure (on 
$[0,\infty)^d$) or the normalized Haar measure on the torus, we write 
$\disc^\star(\bx^N)$ in place of $\disc^\star(\bx^N, \nu)$. 

Sometimes the sequence $\bx$ will not be indexed by the prime numbers, but 
rather by some other discrete subset of $\bR^+$. In that case we will still 
use the notations $\bx^N$, $\bx_{\geqslant N}$, etc., keeping in mind that 
$\pi(N)$ is replaced by $\#\{\textnormal{indices }\leqslant N\}$. 





\section{Examples}
Todo: give some basic examples of equidistributed sequences, 


\begin{definition}
The \emph{van der Corput sequence} is given by 
$\{\frac 1 2,\frac 1 4,\frac 3 4,\dots\}$. More precisely, write $n$ in base 
$2$ as $n = \sum a_i 2^i$. Then $x_n = \sum a_i 2^{-(i+1)}$. 
\end{definition}

The van der Corput sequence has generalizations to other bases and higher 
dimensions. It is well-known for being ``very equidistributed''---i.e.~, its' 
discrepancy has extremely fast convergence to zero. 

\begin{lemma}
We have $\disc(\{x_n\}_{n\leqslant N}) \leqslant \frac{\log(N+1)}{N\log 2}$. 
\end{lemma}
\begin{proof}
This is \cite[Ch.~2 Th.~3.5]{kuipers-niederreiter-1974}. 
\end{proof}

Now, the van der Corput sequence is uniformly distributed, but there is a 
convenient trick to construct sequences equidistributed with respect to more 
general measures. 

\begin{theorem}\label{thm:van-der-corput}
Let $\mu$ be a measure on a closed interval $[a,b]$ such that the cumulative 
distribution function $\cdf_\mu(x) = \mu[a,x]$ is continuous and strictly 
increasing. Then there exists a sequence $\bx=(x_1,x_2,\dots)$ such that 
$\disc(\bx^N,\mu) \ll \frac{\log(N)}{N}$. 
\end{theorem}
\begin{proof}
Since $\cdf_\mu$ is a continuous bijection and its domain is a compact set, 
$\cdf_\mu$ is an order isomorphism. Then Lemma \ref{lem:push-discrepancy} tells 
us that for $\bv$ the van der Corput sequence on $[0,1]$, we have 
$\disc(\cdf_\mu^{-1}(\bv)^N, \mu) = \disc(\bv^N,\mu)$, which gives us the 
desired result with $\bx = \cdf_\mu^{-1}(\bv)$. 
\end{proof}

\begin{theorem}\label{thm:discrepancy-arbitrary}
Let $\mu$ be a measure on $[a,b]$ such that $\cdf_\mu$ is continuous, 
sends $a$ to $0$, and strictly increasing. Fix $\alpha\in (0,1)$. Then there 
exists a sequence $\bx=(x_1,x_2,\dots)$ such that 
$\disc(\bx^N,\mu) = \Theta(N^{-\alpha})$. 
\end{theorem}
\begin{proof}
If $\bx_{\leqslant N}$ is a sequence of length $N$, let 
$\bx_{\leqslant N}:a_{\leqslant M}$ be the sequence $x_1,\dots,x_N,a,\dots,a$ 
($M$ copies of $a$). Then 
\[
	\disc^\star(\bx^N:a^M,\mu)
		\geqslant \left| \frac{\#\{ n\leqslant N+M : x_n = a\}}{N+M} - \mu\{a\}\right| 
		\geqslant \frac{M}{N+M} .
\]
On the other hand, 
\begin{align*}
	\left| \cdf_{N,M}(t) - \cdf_N(t)\right| 
		&\leqslant \frac{\left|\#\{n\leqslant N : x_n\leqslant t\} + M - \frac{M+N}{N}\#\{n\leqslant N : x_n\leqslant t\}\right|}{M+N} \\
%		&\leqslant \frac{\left|M - \frac{M}{N} \#\{n\leqslant N : x_n \leqslant t\} \right|}{M+N} \\
		&\leqslant \frac{2M}{M+N} ,
\end{align*}
which implies that 
$\disc^\star\left(\bx^N:a^M,\mu\right) \leqslant \disc^\star\left(\bx^N,\mu\right) + \frac{2 M}{M+N}$. 
Let $\bv$ be the $\mu$-equidistributed van der Corput sequence of 
Theorem \ref{thm:van-der-corput}, possibly transformed linearly to lie in 
$[a,b]$. We know that $\disc(\{v_n\}_{n\leqslant N}, \mu) \ll N^{-\alpha}$, 
with the constant in question depending only on $\alpha$. 

We construct the sequence $\bx$ via the following recipe. Start with 
$(x_1 = v_1,x_2 = v_2,\dots)$ until, for some $N_1$, 
$\disc(\bx^{N_1},\mu) < N^{-\alpha}$. Then set $x_{N_1+1} = a$, 
$x_{N_1+2} = a$, \dots, until $\disc(\bx^{N_1+M_1},\mu) > N^{-\alpha}$. 
Then set $x_{N_1+M_1+1} = v_{N_1+1}$, $x_{N_1+M_1+2} = v_{N_1+2}$, \dots, 
until once again $\disc(\bx^{N_1+M_1+N_2},\mu) < N^{-\alpha}$. Repeat 
indefinitely. We will show first, that the two steps are possible, and 
that nowhere does $\disc(\bx^N,\mu)$ differ by too much from $N^{-\alpha}$. 

Note that $\frac{M+1}{N+M+1} - \frac{M}{N+M} \leqslant N^{-1}$. This tells 
us that when we are adding $a$s at the end of $\bx^N$, the discrepancy of 
$\bx_{\leqslant N},a_{\leqslant M}$ increases by at most $N^{-1}$ at each 
step. So if $\disc(\bx^N,\mu) < N^{-\alpha}$, we can ensure that 
$\disc(\bx^N:a^M,\mu)$ is at most $N^{-1}$ greater than $N^{-\alpha}$. 

Moreover, we know that $\disc(\bx^N:a,\mu)$ is at most 
$\frac{2}{N+1}$ away from $\disc(\bx^N,\mu)$. So when adding van der Corput 
elements to the end of the sequence, its' discrepancy cannot decay any faster 
than by $\frac{2}{N+1}$ per $a$ added. This yields 
\[
	\left|\disc(\bx^N,\mu) - N^{-\alpha}\right| \ll N^{-1} , 
\]
which is even stronger than we need.
\end{proof}





\section{The Koksma--Hlawka inequality}

Here we summarize the results of the paper \cite{okten-1999}, generalizing them 
as needed for our context. Recall that a function $f$ on $[0,\infty)^d$ is 
said to be of \emph{bounded variation} if there is a finite Radon measure $\nu$ 
such that $f(x) - f(0) = \nu[0,x]$. In such a case we write 
$\Var(f) = |\nu|$. If the appropriate differentiability conditions are 
satisfied, then 
\[
	\Var(f) = \int_{[0,\infty)^d} \left|\frac{\dd^d f}{\dd x_1 \dots \dd x_d} \right|.
\]

\begin{theorem}[Koksma--Hlawka]
Let $\mu$ be a probability measure on $[0,\infty)^d$, $f$ a function of 
bounded variation. Then for any sequence $\bx$ in $[0,\infty)^d$, we have 
\[
	\left| \frac{1}{\pi(x)} \sum_{p\leqslant x} f(x_p) - \int f\, \dd\mu \right| \leqslant \Var(f) \disc(\bx^N,\mu) .
\]
\end{theorem}
\begin{proof}
By our assumptions there is a Radon measure $\nu$ such that 
$f(y) - f(0) = \nu[0,y]$. What follows is essentially trivial, noting that 
$1_{[0,x]}(y) = 1_{[y,\infty)^d}(x)$. 
\begin{align*}
	\frac{1}{\pi(x)} \sum_{p\leqslant x} f(x_p) - \int f\, \dd\mu 
		&= \frac{1}{\pi(x)} \sum_{p\leqslant x} \left(f(x_p) - f(0)\right) - \int \left(f - f(0)\right)\, \dd\mu \\
		&= \frac{1}{\pi(x)} \sum_{p\leqslant x} \int 1_{[y,\infty)^d}(x_p)\, \dd \nu(y) - \int \int 1_{[0,y]}\, \dd\nu \, \dd\mu(y) \\
		&= \int \frac{1}{\pi(x)} \sum_{p\leqslant x} 1_{[y,\infty)^d}(x_p) - \int 1_{[y,\infty)^d}\, \dd\mu \, \dd\nu(y)
\end{align*}
It follows that 
\[
	\left| \frac{1}{\pi(x)} \sum_{p\leqslant x} f(x_p) - \int f\, \dd\mu \right|
		\leqslant \sup_{y\in [0,\infty)} \left| \frac{1}{\pi(x)} \sum_{p\leqslant x} 1_{[y,\infty)}(x_p) - \int 1_{[y,\infty)}\, \dd\mu\right| \cdot |\nu| .
\]
The supremum in question is clearly bounded above by $\disc(\bx^N,\mu)$, so the 
proof is complete. 
\end{proof}





\section{Comparing sequences}

\begin{lemma}
Let $\bx$ and $\by$ be sequences in $[0,\infty)$. Suppose 
$\nu = f\cdot \lambda$ for $f$ a bounded continuous function and $\lambda$ the 
Lebesgue measure. Then 
\[
	\left|\disc^\star(\bx^N, \nu) - \disc^\star(\by^N,\nu)\right| \leqslant \|f\|_\infty \epsilon + \frac{\#\{ n\leqslant N : |x_n - y_n| \geqslant \epsilon\}}{N} .
\]
\end{lemma}
\begin{proof}
Let $\epsilon>0$ and $t\in [0,\infty)$ be arbitrary. For all $n\leqslant N$ 
such that $y_n<t$, either $x_n < t+\epsilon$ or 
$|x_n - y_n| \geqslant \epsilon$. It follows that 
\[
	\by^N[0,t) \leqslant \bx^N[0,t+\epsilon) + \frac{\#\{ n\leqslant N : |x_n - y_n| \geqslant \epsilon\}}{N} .
\]
Moreover, we trivially have 
$\left| \bx^N[0,t+\epsilon) - \nu[0,t+\epsilon)\right| \leqslant \disc^\star(\bx^N,\nu)$. Putting these together, we get: 
\begin{align*}
	\by^N[0,t) - \nu[0,t) 
		&\leqslant \bx^N[0,t+\epsilon) - \nu[0,t) + \frac{\#\{ n\leqslant N : |x_n - y_n| \geqslant \epsilon\}}{N} \\
		&\leqslant \nu[t,t+\epsilon) + \disc^\star(\bx^N,\nu) + \frac{\#\{ n\leqslant N : |x_n - y_n| \geqslant \epsilon\}}{N} \\
		&\leqslant \|f\|_\infty \epsilon + \disc^\star(\bx^N,\nu) + \frac{\#\{ n\leqslant N : |x_n - y_n| \geqslant \epsilon\}}{N} 
\end{align*}
This tells us that 
\[
	\disc^\star(\by^N,\nu) \leqslant \|f\|_\infty \epsilon + \disc^\star(\bx^N,\nu) + \frac{\#\{ n\leqslant N : |x_n - y_n| \geqslant \epsilon\}}{N} .
\]
Reversing the roles of $\bx$ and $\by$, we obtain the desired result. 
\end{proof}

\begin{lemma}
Let $\sigma$ be an isometry of $\bR$, and $\bx$ a sequence in $[0,\infty)$ 
such that $\sigma(\bx)$ is also in $[0,\infty)$. Let $\nu$ be an absolutely 
continuous measure on $[0,\infty)$ such that $\sigma_\ast \nu$ is also 
supported on $[0,\infty)$. Then 
\[
	\left|\disc(\bx^N, \nu) - \disc(\sigma_\ast \bx^N, \sigma_\ast \nu)\right| \leqslant \frac{2}{\pi(N)} .
\]
\end{lemma}
\begin{proof}
Every isometry of $\bR$ is a combination of translations and reflections. 
The statement is clear with translations (the two discrepancies are equal). So, 
suppose $\sigma(t) = a - t$ for some $a>0$. Since $\nu$ is absolutely 
continuous, $\nu\{t\}=0$ for all $t\geqslant 0$. In particular, 
$\nu[s,t) = \nu(s,t]$. In contrast, $\bx^N\{t\}\leqslant \pi(N)^{-1}$. For any 
interval $[s,t)$ in $[0,\infty)$, we know that 
\[
	\left| \bx^N[s,t) - \bx^N(s,t]\right| \leqslant \frac{2}{\pi(N)}  ,
\]
hence 
\[
	\left| \bx^N[s,t) - \nu[s,t) - (\sigma_\ast \bx^N)[a-t,a-s) - (\sigma_\ast \nu)[a-t,a-s)\right| \leqslant \frac{2}{\pi(N)} .
\]
This proves the result. 
\end{proof}

A trick we will use throughout this thesis involves comparing the discrepancy 
of a sequence with the discrepancy of a pushforward sequence, with respect to 
the pushforward measure. 

\begin{lemma}\label{lem:push-discrepancy}
Let $f$ be an order isomorphism $f\colon [a,b] \to [c,d]$. If 
$\bx$ is a sequence on $[a,b]$ and $\mu$ is a probability measure on 
$[a,b]$, then 
$\disc(\bx^N, \mu) = \disc(f(\bx)^N, f_\ast \mu)$, 
and likewise for star discrepancy. 
\end{lemma}
\begin{proof}
This is a simple computation, which we only check for star discrepancy: 
\begin{align*}
	\disc(f(\bx)^N, f_\ast \mu)
		&= \sup_{t\in [c,d]} \left| \frac{\#\{n\leqslant N : f(x_n) \leqslant t\}}{N} - (f_\ast \mu)[a,t]\right| \\
		&= \sup_{t\in [a,b]} \left| \frac{\#\{n\leqslant N : x_n \leqslant f^{-1}(t)\}}{N} - \mu[a,f^{-1}(t)]\right| \\
		&= \disc(\bx^N, \mu) .
\end{align*}
\end{proof}





\section{Combining sequences}

\begin{definition}
Let $\bx$ and $\by$ be sequences in $[0,\infty)^d$. We write $\bx\wr\by$ for 
the interleaved sequence 
\[
	(x_2,y_2,x_3,y_3,x_5,y_5,\dots,x_p,y_p,\dots) .
\]
\end{definition}

For the interleaved sequence $\bx\wr\by$, we write $(\bx\wr\by)^N$ for the 
empirical measure 
\[
	(\bx\wr\by)^N = \frac{1}{2\pi(N)} \sum_{p\leqslant N} \delta_{x_p} + \delta_{y_p} .
\]

\begin{theorem}
Let $I$ and $J$ be disjoint open boxes in $[0,\infty)^d$, and let $\mu$, 
$\nu$ be absolutely continuous probability measures on $I$ and $J$, 
respectively. Let $\bx$ be a sequence in $I$ and $\by$ be a sequence in $J$. 
Then 
\[
	\max\{\disc(\bx^N,\mu),\disc(\by^N,\nu)\} \leqslant \disc((\bx\wr\by)^N, \mu+\nu) \leqslant \disc(\bx^N,\mu) + \disc(\by^N,\nu)
\]
\end{theorem}
\begin{proof}
Any half-open box in $[0,\infty)^d$ can be split by a coordinate 
hyperplane into two disjoint half-open boxes $[a,b)\sqcup [s,t)$, each of which 
intersects at most one of $I$ and $J$. We may assume that 
$[a,b)\cap J=\varnothing$ and $[s,t)\cap I = \varnothing$. Then 
\begin{align*}
	\left| (\bx\wr\by)^N([a,b)\sqcup [s,t)) - (\mu+\nu)([a,b)\sqcup[s,t))\right| 
		&\leqslant |\bx^N[a,b) - \mu[a,b)| + |\by^N[s,t) - \nu[s,t)| \\
		&\leqslant \disc(\bx^N,\mu) + \disc(\by^N,\nu) .
\end{align*}
This yields the second inequality in the statement of the theorem. To see the 
first, assume that the maximum discrepancy is $\disc(\bx^N,\mu)$, and let 
$[s,t)$ be a half-open box such that $|\bx^N[s,t) - \mu[s,t)|$ is within an 
arbitrary $\epsilon$ of $\disc(\bx^N,\mu)$. We can assume that $[s,t)$ does not 
intersect $J$, and thus 
\[
	\left|(\bx\wr\by)^N[s,t) - (\mu+\nu)[s,t)\right| = |\bx^N[s,t) - \mu[s,t)| ,
\]
which yields the result. 
\end{proof}
