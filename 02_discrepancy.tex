% !TEX root = main.tex

\chapter{Discrepancy}





\section{Equidistribution}

The discrepancy (also known as the Kolmogorov--Smirnov statistic) is a way of 
measuring how closely sample data fits a predicted distribution. It has many 
applications in computer science and statistics, but here we will focus on only 
the basic known properties, as well as how discrepancy changes when sequences 
are tweaked and/or combined. 

First, recall that the discrepancy is a way of sharpening the ``soft'' 
convergence results of, say \cite[A.1]{serre-1989}. Let $X$ be a compact 
topological space, $\{x_p\}$ a sequence of points in $X$ indexed by the prime 
numbers. 

\begin{definition}
Let $\mu$ be a continuous probability measure on $X$. The sequence $\{x_p\}$ is 
\emph{equidistributed} with respect to $\mu$ if for all $f\in C(X)$, we have 
\[
	\lim_{x\to \infty} \frac{1}{\pi(x)} \sum_{p\leqslant x} f(x_p) \to \int f\, \dd \mu .
\]
\end{definition}

In other words, $\{x_p\}$ is $\mu$-equidistributed if the empirical measures 
$P_x = \frac{1}{\pi(x)} \sum_{p\leqslant x} \delta_{x_p}$ converge to $\mu$ in 
the weak topology. It is easy to see that $\{x_p\}$ is $\mu$-equidistributed if 
and only if $\left| \sum_{p\leqslant x} f(x_p)\right| = o(x)$ for all 
continuous $f$ having $\int f\, \dd\mu = 0$. In fact, one can restrict to a 
set of $f$ which generate a dense subpace of $C(X)^{\mu =0}$. 

In the discussion in \cite[A.1]{serre-1989}, $X$ is the space of conjugacy 
classes in a compact Lie group, and $f$ is allowed to range over the characters 
of irreducible, nontrivial representations of the group. In this section, we 
will show that the entire discussion can be generalized to a much broader class 
of \emph{strange Dirichlet series}, which are of the form 
\[
	L_f(\{x_p\},s) = \prod_p \frac{1}{1-f(x_p)p^{-s}} .
\]
A useful, but not too well known, result, is that we in fact can consider 
functions $f$ which are only continuous almost everywhere. 

\begin{theorem}
Let $X$ be a compact separable metric space with no isolated points. Let $\mu$ 
be a Borel measure on $X$ and let $f\colon X\to \bC$ be bounded and measurable. 
Then $f$ is continuous almost everywhere if and only if 
\[
	\lim_{x\to \infty} \frac{1}{\pi(x)} \sum_{p\leqslant x} f(x_p) = \int f\, \dd\mu
\]
for all $\mu$-equidistributed sequences $\{x_p\}$. 
\end{theorem}
\begin{proof}
This follows immediately from the proof of \cite[Th.~1]{mazzone-1995}
\end{proof}





\section{Definitions and first results}

We will define discrepancy for measures on the $d$-dimensional half-open box 
$[0,\infty)^d$. For vectors $x,y\in [0,\infty)^d$, we say $x<y$ if 
$x_1<y_1$,\dots,$x_d<y_d$, and in that case write $[x,y)$ for the half-open 
box $[x_1,y_1)\times \cdots \times [x_d,y_d)$. 

\begin{definition}
Let $\mu, \nu$ be probability measures on $[0,\infty)^d$. The 
\emph{discrepancy} of $\mu$ with respect to $\nu$ is 
\[
	\disc(\mu,\nu) = \sup_{x < y} \left| \mu[x,y) - \nu[x,y)\right| ,
\]
where $x<y$ range over $[0,\infty)^d$.

The \emph{star discrepancy} of $\mu$ with respect to $\nu$ is 
\[
	\disc^\star(\mu,\nu) = \sup_{0<y} \left| \mu[0,y) - \nu[0,y)\right| ,
\]
where $y$ ranges over $[0,\infty)^d$. 
\end{definition}

\begin{lemma}
Let $\mu,\nu$ be Borel measures on $\bR^d$. Then 
\[
	\disc^\star(\mu,\nu) \leqslant \disc(\mu,\nu) \leqslant 2^d \disc^\star(\mu,\nu) .
\]
\end{lemma}
\begin{proof}
The first inequality holds because the supremum defining the discrepancy is 
taken over a larger set than that defining star discrepancy. To prove the 
second inequality, let $x<y$ be in $[0,\infty)^d$. For 
$S\subset \{1,\dots,d\}$, let 
\[
	I_S = \{ t \in [0,y) : t_i < x_i \text{ for all }i\in S\} .
\]
The inclusion-exclusion principle for measures tells us that: 
\[
	\mu[x,y) = \sum_{S\subset \{1,\dots,d\}} (-1)^{\# S} \mu(I_S) ,
\]
and similarly for $\nu$. Since each of the $I_S$ are ``half-open boxes'' 
we know that $|\mu(I_S) - \nu(I_S)| \leqslant \disc^\star(\mu,\nu)$. It 
follows that 
\[
	|\mu[x,y) - \nu[x,y)| \leqslant \sum_{S\subset \{1,\dots,d\}} |\mu(I_S) - \nu(I_S)| \leqslant 2^d \disc^\star(\mu,\nu) .
\]
For a discussion and related context, see 
\cite[Ch.~2 Ex.~1.2]{kuipers-niederreiter-1974}. 
\end{proof}

We are usually interested in comparing empirical measures and their conjectured 
distribution. Namely, let $\bx = \{x_p\}$ be a sequence in $[0,\infty)^d$ 
indexed by the prime numbers, and $\mu$ a Borel measure on $[0\infty)^d$. For 
any real number $N\geqslant 2$, we write $\bx^N$ for the empirical measure 
given by 
\[
	\bx^N(S) = \frac{1}{\pi(N)} \sum_{p\leqslant N} \delta_{x_p}(S) = \frac{\# \{p\leqslant N : x_p\in S\}}{\pi(N)} .
\]
Also, we write $\bx_{\geqslant N}$ for the truncated sequence 
$(x_p)_{p\geqslant N}$, and similarly for $\bx_{\leqslant N}$, etc. In this 
context, 
\[
	\disc^\star(\bx^N,\nu) = \sup_{y\in [0,\infty)^d} \left| \frac{\# \{p\leqslant N : x_p \in [0,y)\}}{\pi(N)} - \int_{[0,y)} \, \dd\nu\right| .
\]

If the measure $\nu$ is only defined on a subset of $[0,\infty)^d$, we will 
tacitly extend it by zero. Moreover, if the sequence $\bx$ actually lies in a 
torus $(\bR/a \bZ)^d$, we identify that torus with the 
$[0,a)^d\subset [0,\infty)^d$. If $\nu$ is the Lebesgue measure (on 
$[0,\infty)^d$) or the normalized Haar measure on the torus, we write 
$\disc^\star(\bx^N)$ in place of $\disc^\star(\bx^N, \nu)$. 

Sometimes the sequence $\bx$ will not be indexed by the prime numbers, but 
rather by some other discrete subset of $\bR^+$. In that case we will still 
use the notations $\bx^N$, $\bx_{\geqslant N}$, etc., keeping in mind that 
$\pi(N)$ is replaced by $\#\{\textnormal{indices }\leqslant N\}$. 

Todo: give some basic examples of equidistributed sequences, talk about 
equidistribution, almost-everywhere continuous functions. Prove basic facts 
about van der Corput sequence for arbitrary measures. 





\section{The Koksma--Hlawka inequality}

Here we summarize the results of the paper \cite{okten-1999}, generalizing them 
as needed for our context. Recall that a function $f$ on $[0,\infty)^d$ is 
said to be of \emph{bounded variation} if there is a finite Radon measure $\nu$ 
such that $f(x) - f(0) = \nu[0,x]$. In such a case we write 
$\Var(f) = |\nu|$. If the appropriate differentiability conditions are 
satisfied, then 
\[
	\Var(f) = \int_{[0,\infty)^d} \left|\frac{\dd^d f}{\dd x_1 \dots \dd x_d} \right|.
\]

\begin{theorem}[Koksma--Hlawka]
Let $\mu$ be a probability measure on $[0,\infty)^d$, $f$ a function of 
bounded variation. Then for any sequence $\bx$ in $[0,\infty)^d$, we have 
\[
	\left| \frac{1}{\pi(x)} \sum_{p\leqslant x} f(x_p) - \int f\, \dd\mu \right| \leqslant \Var(f) \disc(\bx^N,\mu) .
\]
\end{theorem}
\begin{proof}
By our assumptions there is a Radon measure $\nu$ such that 
$f(y) - f(0) = \nu[0,y]$. What follows is essentially trivial, noting that 
$1_{[0,x]}(y) = 1_{[y,\infty)^d}(x)$. 
\begin{align*}
	\frac{1}{\pi(x)} \sum_{p\leqslant x} f(x_p) - \int f\, \dd\mu 
		&= \frac{1}{\pi(x)} \sum_{p\leqslant x} \left(f(x_p) - f(0)\right) - \int \left(f - f(0)\right)\, \dd\mu \\
		&= \frac{1}{\pi(x)} \sum_{p\leqslant x} \int 1_{[y,\infty)^d}(x_p)\, \dd \nu(y) - \int \int 1_{[0,y]}\, \dd\nu \, \dd\mu(y) \\
		&= \int \frac{1}{\pi(x)} \sum_{p\leqslant x} 1_{[y,\infty)^d}(x_p) - \int 1_{[y,\infty)^d}\, \dd\mu \, \dd\nu(y)
\end{align*}
It follows that 
\[
	\left| \frac{1}{\pi(x)} \sum_{p\leqslant x} f(x_p) - \int f\, \dd\mu \right|
		\leqslant \sup_{y\in [0,\infty)} \left| \frac{1}{\pi(x)} \sum_{p\leqslant x} 1_{[y,\infty)}(x_p) - \int 1_{[y,\infty)}\, \dd\mu\right| \cdot |\nu| .
\]
The supremum in question is clearly bounded above by $\disc(\bx^N,\mu)$, so the 
proof is complete. 
\end{proof}





\section{Comparing sequences}

\begin{lemma}
Let $\bx$ and $\by$ be sequences in $[0,\infty)$. Suppose 
$\nu = f\cdot \lambda$ for $f$ a bounded continuous function and $\lambda$ the 
Lebesgue measure. Then 
\[
	\left|\disc^\star(\bx^N, \nu) - \disc^\star(\by^N,\nu)\right| \leqslant \|f\|_\infty \epsilon + \disc^\star(\bx^N,\nu) + \frac{\#\{ p\leqslant N : \|x_p - y_p\|_\infty \geqslant \epsilon\}}{\pi(N)} .
\]
\end{lemma}
\begin{proof}
Let $\epsilon>0$ and $t\in [0,\infty)$ be arbitrary. For all $p\leqslant N$ 
such that $y_p<t$, either $x_p < t+\epsilon$ or 
$\|x_p - y_p\|_\infty \geqslant \epsilon$. It follows that 
\[
	\by^N[0,t) \leqslant \bx^N[0,t+\epsilon) + \frac{\#\{ p\leqslant N : \|x_p - y_p\|_\infty \geqslant \epsilon\}}{\pi(N)} .
\]
Moreover, we trivially have 
\[
	\left| \bx^N[0,t+\epsilon) - \nu[0,t+\epsilon)\right| \leqslant \disc^\star(\bx^N,\nu) .
\]
Putting these together, we get: 
\begin{align*}
	\by^N[0,t) - \nu[0,t) 
		&\leqslant \bx^N[0,t+\epsilon) - \nu[0,t) + \frac{\#\{ p\leqslant N : \|x_p - y_p\|_\infty \geqslant \epsilon\}}{\pi(N)} \\
		&\leqslant \nu[t,t+\epsilon) + \disc^\star(\bx^N,\nu) + \frac{\#\{ p\leqslant N : \|x_p - y_p\|_\infty \geqslant \epsilon\}}{\pi(N)} \\
		&\leqslant \|f\|_\infty \epsilon + \disc^\star(\bx^N,\nu) + \frac{\#\{ p\leqslant N : \|x_p - y_p\|_\infty \geqslant \epsilon\}}{\pi(N)} 
\end{align*}
as desired. 
\end{proof}

\begin{lemma}
Let $\sigma$ be an isometry of $\bR$, and $\bx$ a sequence in $[0,\infty)$ 
such that $\sigma(\bx)$ is also in $[0,\infty)$. Let $\nu$ be an absolutely 
continuous measure on $[0,\infty)$ such that $\sigma_\ast \nu$ is also 
supported on $[0,\infty)$. Then 
\[
	\left|\disc(\bx^N, \nu) - \disc(\sigma_\ast \bx^N, \sigma_\ast \nu)\right| \leqslant \frac{2}{\pi(N)} .
\]
\end{lemma}
\begin{proof}
Every isometry of $\bR$ is a combination of translations and reflections. 
The statement is clear with translations (the two discrepancies are equal). So, 
suppose $\sigma(t) = a - t$ for some $a>0$. Since $\nu$ is absolutely 
continuous, $\nu\{t\}=0$ for all $t\geqslant 0$. In particular, 
$\nu[s,t) = \nu(s,t]$. In contrast, $\bx^N\{t\}\leqslant \pi(N)^{-1}$. For any 
interval $[s,t)$ in $[0,\infty)$, we know that 
\[
	\left| \bx^N[s,t) - \bx^N(s,t]\right| \leqslant \frac{2}{\pi(N)}  ,
\]
hence 
\[
	\left| \bx^N[s,t) - \nu[s,t) - (\sigma_\ast \bx^N)[a-t,a-s) - (\sigma_\ast \nu)[a-t,a-s)\right| \leqslant \frac{2}{\pi(N)} .
\]
This proves the result. 
\end{proof}





\section{Combining sequences}

\begin{definition}
Let $\bx$ and $\by$ be sequences in $[0,\infty)^d$. We write $\bx\wr\by$ for 
the interleaved sequence 
\[
	(x_2,y_2,x_3,y_3,x_5,y_5,\dots,x_p,y_p,\dots) .
\]
\end{definition}

For the interleaved sequence $\bx\wr\by$, we write $(\bx\wr\by)^N$ for the 
empirical measure 
\[
	(\bx\wr\by)^N = \frac{1}{2\pi(N)} \sum_{p\leqslant N} \delta_{x_p} + \delta_{y_p} .
\]

\begin{theorem}
Let $I$ and $J$ be disjoint open boxes in $[0,\infty)^d$, and let $\mu$, 
$\nu$ be absolutely continuous probability measures on $I$ and $J$, 
respectively. Let $\bx$ be a sequence in $I$ and $\by$ be a sequence in $J$. 
Then 
\[
	\max\{\disc(\bx^N,\mu),\disc(\by^N,\nu)\} \leqslant \disc((\bx\wr\by)^N, \mu+\nu) \leqslant \disc(\bx^N,\mu) + \disc(\by^N,\nu)
\]
\end{theorem}
\begin{proof}
Any half-open box in $[0,\infty)^d$ can be split by a coordinate 
hyperplane into two disjoint half-open boxes $[a,b)\sqcup [s,t)$, each of which 
intersects at most one of $I$ and $J$. We may assume that 
$[a,b)\cap J=\varnothing$ and $[s,t)\cap I = \varnothing$. Then 
\begin{align*}
	\left| (\bx\wr\by)^N([a,b)\sqcup [s,t)) - (\mu+\nu)([a,b)\sqcup[s,t))\right| 
		&\leqslant |\bx^N[a,b) - \mu[a,b)| + |\by^N[s,t) - \nu[s,t)| \\
		&\leqslant \disc(\bx^N,\mu) + \disc(\by^N,\nu) .
\end{align*}
This yields the second inequality in the statement of the theorem. To see the 
first, assume that the maximum discrepancy is $\disc(\bx^N,\mu)$, and let 
$[s,t)$ be a half-open box such that $|\bx^N[s,t) - \mu[s,t)|$ is within an 
arbitrary $\epsilon$ of $\disc(\bx^N,\mu)$. We can assume that $[s,t)$ does not 
intersect $J$, and thus 
\[
	\left|(\bx\wr\by)^N[s,t) - (\mu+\nu)[s,t)\right| = |\bx^N[s,t) - \mu[s,t)| ,
\]
which yields the result. 
\end{proof}
